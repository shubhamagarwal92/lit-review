## Visual Question Answering 

(Last updated: 20th April 2020)

### VQAv2 

- [Website](https://visualqa.org)
- [Challenge](https://visualqa.org/challenge.html)

| Model           | Accuracy  |  Paper / Source | Comments | Conference |
| ------------- | :-----:| --- | ---| --- |
| LXMERT (Tan et al., 2019) | 72.54 | [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://github.com/airsplay/lxmert) | Published SOTA | EMNLP'19 |
| UNITER (Chen et al., 2019) | 73.4 | [UNITER: LEARNING UNIVERSAL IMAGE-TEXT REPRESENTATIONS](https://arxiv.org/pdf/1909.11740.pdf) | Unpublished (ICLR'20 submission) | - |


### GQA - Visual Reasoning in the Real World 

- [Website](https://cs.stanford.edu/people/dorarad/gqa/)
- [Challenge](https://cs.stanford.edu/people/dorarad/gqa/challenge.html)

| Model           | Accuracy  |  Paper / Source | Comments | Conference |
| ------------- | :-----:| --- | ---| --- |
| LXMERT (Tan et al., 2019) | 60.3 | [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://github.com/airsplay/lxmert) | Published SOTA | [EMNLP'19](https://arxiv.org/abs/1908.07490) |
| KaKao Brain | 73.24 | [GQA Challenge](https://drive.google.com/file/d/1CtFk0ldbN5w2qhwvfKrNzAFEj-I9Tjgy/view) | Challenge Results |  |

### TextVQA

- [Website](https://textvqa.org/)
- [Challenge](https://textvqa.org/challenge)

| Model           | Accuracy  |  Paper / Source | Comments | Conference |
| ------------- | :-----:| --- |---| --- |
| M4C (Hu et al., 2020) | 40.46 | [Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA](https://arxiv.org/pdf/1911.06258.pdf) | [Baseline by organizers](https://github.com/facebookresearch/pythia/tree/project/m4c/projects/M4C_Captioner) | |


### VizWiz dataset

- [Website](https://vizwiz.org/tasks-and-datasets/vqa/)
- [Challenge](https://vizwiz.org/tasks-and-datasets/vqa/)

| Model           | Accuracy  |  Paper / Source | Comments | Conference |
| ------------- | :-----:| --- | ---| --- |
| BUTD Vizwiz (Gurari et al., 2018) | 46.9 | [VizWiz Grand Challenge: Answering Visual Questions from Blind People](https://arxiv.org/abs/1802.08218) | Published SOTA | CVPR'18 |
| Pythia | 54.22 | [FB's Pythia repository](https://github.com/facebookresearch/pythia/blob/master/docs/source/tutorials/pretrained_models.md) | Unofficial SOTA |  |


### Document VQA 

- [Website](https://rrc.cvc.uab.es/?ch=17&com=contact)

### FigurQA and DVQA

- [DVQA](https://arxiv.org/pdf/1801.08163.pdf) by Adobe Research
- [FigureQA](https://arxiv.org/pdf/1710.07300.pdf)

### OCR-VQA

- [Paper](https://anandmishra22.github.io/files/mishra-OCR-VQA.pdf)

### ST-VQA (Scene Text Visual Question Answering)

- [Paper](http://openaccess.thecvf.com/content_ICCV_2019/papers/Biten_Scene_Text_Visual_Question_Answering_ICCV_2019_paper.pdf)

### IQA: Visual Question Answering in Interactive Environments

- [Paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Gordon_IQA_Visual_Question_CVPR_2018_paper.pdf)

### RecipeQA

- [Paper](https://arxiv.org/pdf/1809.00812.pdf)

## VideoQA

### MSVD-QA and MSRVTT-QA

- [Paper](http://staff.ustc.edu.cn/~hexn/papers/mm17-videoQA.pdf)

## Other multimodal resources

- [awesome-multimodal-ml](https://github.com/pliang279/awesome-multimodal-ml)
- [awesome-vision-and-language-papers](https://github.com/sangminwoo/awesome-vision-and-language-papers)
- [Visual Question Answering and Dialog Workshop](https://visualqa.org/workshop.html)

