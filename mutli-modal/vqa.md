## Visual Question Answering

### VQAv2 

- [Website](https://visualqa.org)
- [Challenge](https://visualqa.org/challenge.html)

| Model           | Accuracy  |  Paper / Source | Comments | Conference |
| ------------- | :-----:| --- | ---| --- |
| LXMERT (Tan et al., 2019) | 72.54 | [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://github.com/airsplay/lxmert) | Published SOTA | EMNLP'19 |
| UNITER (Chen et al., 2019) | 73.4 | [UNITER: LEARNING UNIVERSAL IMAGE-TEXT REPRESENTATIONS](https://arxiv.org/pdf/1909.11740.pdf) | Unpublished (ICLR'20 submission) | - |


### GQA - Visual Reasoning in the Real World 

- [Website](https://cs.stanford.edu/people/dorarad/gqa/)
- [Challenge](https://cs.stanford.edu/people/dorarad/gqa/challenge.html)

| Model           | Accuracy  |  Paper / Source | Comments | Conference |
| ------------- | :-----:| --- | ---| --- |
| LXMERT (Tan et al., 2019) | 60.3 | [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://github.com/airsplay/lxmert) | Published SOTA | EMNLP'19 |
| KaKao Brain | 73.24 | [GQA Challenge](https://drive.google.com/file/d/1CtFk0ldbN5w2qhwvfKrNzAFEj-I9Tjgy/view) | Challenge Results |  |

### TextVQA

- [Website](https://textvqa.org/)
- [Challenge](https://textvqa.org/challenge)

| Model           | Accuracy  |  Paper / Source | Comments | Conference |
| ------------- | :-----:| --- |---| --- |
| M4C (Hu et al., 2020) | 40.46 | [Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA](https://arxiv.org/pdf/1911.06258.pdf) | Baseline by organizers | |


### VizWiz dataset

- [Website](https://vizwiz.org/tasks-and-datasets/vqa/)
- [Challenge](https://vizwiz.org/tasks-and-datasets/vqa/)

| Model           | Accuracy  |  Paper / Source | Comments | Conference |
| ------------- | :-----:| --- | ---| --- |
| BUTD Vizwiz (Gurari et al., 2018) | 46.9 | [VizWiz Grand Challenge: Answering Visual Questions from Blind People](https://arxiv.org/abs/1802.08218) | Published SOTA | CVPR'18 |
| Pythia | 54.22 | [FB's Pythia repository](https://github.com/facebookresearch/pythia/blob/master/docs/source/tutorials/pretrained_models.md) | Unofficial SOTA |  |

## Other multimodal resources

- [awesome-multimodal-ml](https://github.com/pliang279/awesome-multimodal-ml)
- [awesome-vision-and-language-papers](https://github.com/sangminwoo/awesome-vision-and-language-papers)
- [Visual Question Answering and Dialog Workshop](https://visualqa.org/workshop.html)

